# **机器学习笔记**

## 机器学习概述

### 1 相关概述

1. 人工智能（Artificial Intelligence）：用计算机模拟人脑，让计算机像人类一样理性的思考和行动
2. 机器学习（Machine Leaning）：通过算法让计算机从数据中学习规律，无需人工编写具体规则，实现自主决策、预测或模式识别
   - 数据和特征决定了机器学习的上限，而模型和算法只是在逼近这个上限
3. 深度学习（Deep Leaning）：也叫深度神经网络，大脑仿生，设计一层一层的神经元模拟万事万物
4. AI、ML、DL三者的关系：机器学习是实现人工智能的一种途径，深度学习是机器学习的一种方法发展而来的
5. 数据、算法、算力三者是AI发展的基石

![image-20251117145944356](C:\Users\20538\AppData\Roaming\Typora\typora-user-images\image-20251117145944356.png)



### 2 机器学习算法分类

1. **有监督学习：**输入数据是由输入特征值和目标值所组成，即输入的训练数据有标签的
   - **分类问题**：**目标值（标签值）是不连续的**
   - **回归问题**：**目标值（标签值）是连续的**

2. **无监督学习：** **聚类问题**，输入数据没有被标记，即样本数据类别未知，没有标签，根据样本间的相似性，对样本集聚类，以发现事物内部的结构和相互关系
3. 半监督学习：专家标注少量数据，利用已经标记的数据(也就是带有类标签)训练出一个模型，再利用该模型去套用未标记的数据，再通过询问领域专家分类结果与模型分类结果做对比
4. 强化学习：机器学习的重要分支（寻找最优解），输入动态数据，决策+回报函数
5. ![image-20251118100802423](C:\Users\20538\AppData\Roaming\Typora\typora-user-images\image-20251118100802423.png)

### 3 建模流程

1. **获取数据**

2. **数据基本处理**

3. **特征工程：**利用专业背景知识和技巧处理数据，让机器学习算法效果最好，这恶过程就是特征工程
   
   - **(1) 特征提取：**从原始数据中提取与任务相关的特征
   
   - **(2) 特征预处理：**特征对模型产生影响；因量纲问题，有些特征对模型影响大、有些影响小（鲁棒性）；**做数据的标准化、归一化** 
   
     - **特征预处理**：特征单位或大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）结果，使得一些模型（算法）无法学习到其它的特征
   
       - **归一化：**通过对原始数据进行变换把数据映射到【min,max】（默认[0,1]）之间（适用于小数据集）
   
         - $$
           x'_i = \frac{x_i - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
           $$
   
         - $$
           \begin{equation} X'' = X'*(max - min) + min \end{equation}
           $$
   
       - **标准化：**通过对原始数据进行标准化，转换为均值为0，标准差为1的标准正态分布的数据（适用于大数据集）
   
         - $$
           z = \frac{x - \mu}{\sigma} \quad (\mu=\text{均值},\ \sigma=\text{标准差})
           $$
   
         - 方差计算公式：该列的每个值和该列的均值的差的平方和的平均值
   
         - $$
           \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2
           $$
   
         - 标准差计算公式：方差求平方根
   
         - $$
           \sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}
           $$
   
   - **(3) 特征降维：**将原始数据的维度降低，会修改原数据
   
   - **(4) 特征选择：**原始数据特征很多，但是对模型训练相关是其中一个特征集合子集，不会修改原数据
   
   - **(5) 特征组合：**把多个特征合并成一个特征，一般利用乘法或者加法来完成
   
4. **模型训练**

5. **模型评估**

6. **模型预测**

### 4 拟合问题

1. **拟合**：用在机器学习领域，用来表示模型对样本点的拟合情况，拟合=模型在训练集和测试集上表现情况
2. **欠拟合**：模型在训练集和测试集上表现都不好
3. **过拟合**：训练集好，测试集不好
4. **泛化**：模型在新数据集（非训练数据）上的表现好坏的能力
5. **奥卡姆剃刀原则**：给定两个具有相同泛化误差的模型，较简单的模型比较复杂的模型更可取

### **5 交叉验证和网格搜索**

1. 交叉验证：交叉验证是一种模型评估技术，旨在确保模型的泛化能力。它通过反复训练和测试模型来给出一个更可靠的性能评估

   - 原理：把数据分成n份，例如分成：4份--->也叫4折交叉验证

   - 第1次：把第1份数据作为验证集(测试集)，其它作为训练集，训练模型，模型预测，获取：准确率准确率1

   - 第2次：把第2份数据作为验证集(测试集)，其它作为训练集，训练模型，模型预测，获取：准确率 准确率2

   - 第3次：把第3份数据作为验证集(测试集)，其它作为训练集，训练模型，模型预测，获取：准确率准确率3

   - 第4次：把第4份数据作为验证集(测试集)，其它作为训练集，训练模型，模型预测，获取：准确率准确率4

   - 然后计算上述的4次准确率的平均值，作为：模型最终的准确率

   - 假设第4次最好(准确率最高)，则：用全部数据(训练集+测试集)训练模型，再次用(第4次的)测试集对模型测试

   - 目的:为了让模型的最终验证结果更准确

1. 网格搜索：网格搜索是一种超参数优化技术，用于系统地遍历多个超参数组合，以找到最优的超参数配置，从而提升模型性能

   - 目的/作用：寻找最优超参数.

   - 原理：接收超参可能出现的值，然后针对于超参的每个值进行交叉验证，获取到最优超参组合

## 机器学习相关算法

### 1 KNN算法（K近邻算法）

1. **knn算法思想**：基于待预测样本与训练集中各样本的距离，选取距离最近的 K 个邻居，分类时以邻居中出现最多的类别作为预测类别，回归时以邻居目标值的均值作为预测值 。

   - **解决的问题**：分类问题（投票多数表决）、回归问题（求平均值）
   - 样本相似性：样本都是属于一个任务数据集的。样本距离越近越相似	
   - **K值过小**：用较小邻域中的训练实例进行预测；容易受到异常值的影响，K值的减小就意味着整体模型变得复杂，易发生过拟合
   - **K值过大**：用较大邻域中的训练实例进行预测；受到样本均衡的问题，且K值的增大就意味着整体的模型变得简单，易发生欠拟合

2. KNN分类流程（无监督）：

   - 1.计算未知样本到每一个训练样本的距离
   - 2.将训练样本根据距离大小升序排列
   - 3.取出距离最近的K个训练样本
   - 4.进行多数表决，统计K个样本中哪个类别的样本个数最多
   - 5.将未知的样本归属到出现次数最多的类别

3. KNN回归流程（有监督）：

   - 1.计算未知样本到每一个训练样本的距离
   - 2.将训练样本根据距离大小升序排列
   - 3.取出距离最近的K个训练样本
   - 4.把这个K个样本的目标值计算其平均值
   - 5.将未知的样本预测的值

4. 常用的距离度量方法：

   - **欧几里得距离：**两个点在空间中的距离一般都是欧几里得距离。对应维度差值平方和，开平方根
     $$
     d = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
     $$

   - **曼哈顿距离：**也称城市街区距离，也称为出租车距离，用于衡量两个点在标准坐标系上的绝对轴距总和。对应维度差值的绝对值之和

   - $$
     d(\vec{x}, \vec{y}) = \sum_{i=1}^{n} |x_i - y_i|
     $$

     

   - **切比雪夫距离：**n 维空间两点各坐标差值的最大值，即 ”最大维度距离“

   - $$
     d_{\text{Chebyshev}}(P,Q) = \max_{1 \leq i \leq n}\left(|x_i - y_i|\right)
     $$

   - **闵可夫斯基距离：**n 维空间两点距离的广义形式，含欧氏 / 曼哈顿 / 切比雪夫特例（统一欧氏、曼哈顿、切比雪夫距离，p 值决定距离计算逻辑）

   - $$
     d_{\text{Minkowski}}(P,Q) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{\frac{1}{p}}
     $$

### 2 线性回归	

1. 线性回归的概念：利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式（有监督学习，有特征，有标签，且标签连续（回归））。

   - 多元线性回归公式：

   - $$
     y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + \varepsilon
     $$

     

   - 多元线性回归矩阵公式：

   - $$
     \mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\varepsilon}
     $$

2. **损失函数：**用来描述每个样本点和其预测值之间关系的，让损失函数最小，就是让误差和最小，线性回归效率、评估越高。**损失函数=各个样本的误差和，越小越好（误差 = 预测值 - 真实值）。**

   - **如何让损失函数最小：（1）：正规方程法；（2）：梯度下降法**

   - **损失函数的种类和数学表达式（线性回归模型评估----三种指标）：**

     - **（1）均方误差：（Mean-Square-Error，MSE）**：（每个样本点）误差的平方和 / 样本总数

     - $$
       \text{MSE} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
       $$

     - **（2）平均绝对误差（Mean Absolute Error，MAE）**：（每个样本点）误差的绝对值之和 / 样本总数

     - $$
       \text{MAE} = \frac{1}{m} \sum_{i=1}^{m} \left| \hat{y}^{(i)} - y^{(i)} \right|
       $$

     - **（3）均方根误差（Root Mean Squared Error）：**（每个样本点）误差的平方和 / 样本总数，再开根号

     - $$
       \text{RMSE} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2}
       $$

     - **（4）最小二乘法**：（每个样本点）误差的平方和

     - $$
        J(\mathbf{w}, b) = \sum_{i=0}^{m} \left( h\left( x^{(i)} \right) - y^{(i)} \right)^2
        $$

        

3. 正规方程法：正规方程法是线性回归中求解最优模型参数的解析方法，通过最小化平方损失函数，直接对参数求导并令导数为 0，得到参数的精确解（无需迭代），目的是为了损失函数最小

   - 正规方程法存在的问题：（1）：如果运算量过大，可能造成内存溢出，时间复杂度大；（2）：假设矩阵没有逆，则可能无解

   - 一元线性回归正规方程法

     - 一元线性回归的模型：\(w)是截距项，\(b)是特征x的系数，x是唯一特征，y是标签是截距项，\(w)是特征x的系数，x是唯一特征，y是标签

     - $$
       y = w x + b
       $$

       

   - 多元线性回归正规方程法

     - 多元线性回归的模型为：

     - $$
       y = w_1 x_1 + w_2 x_2 + w_3 x_3 + \cdots + b = \mathbf{w}^T \mathbf{x} + b
       $$

     - 
     
     - $$
       \begin{aligned}
       J(\mathbf{w}) &= \left(h(x_1) - y_1\right)^2 + \left(h(x_2) - y_2\right)^2 + \cdots + \left(h(x_m) - y_m\right)^2 \\
       &= \sum_{i=1}^{m} \left(h(x_i) - y_i\right)^2 = \left\| \mathbf{X}\mathbf{w} - \mathbf{y} \right\|_2^2
       \end{aligned}
       $$
     
     - w解为：
       $$
       \mathbf{w} = \left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{y}
       $$

4. **梯度下降法：沿着梯度下降的方向求解最小值，目的是为了使损失函数最小**

   - 梯度定义：

     - 单变量函数中（一元），梯度就是某一点切线斜率，（某一点的导数）；有方向为函数增长最快的方向
     - 多变量函数中（多元），梯度就是，某一点的偏导数；有方向：偏导数分量的向量方向

   - 梯度下降公式：

     - **循环迭代求当前点的梯度，更新当前的权重参数（下个点 = 当前点 - 学习率 * 损失函数的导数（梯度函数））**

     - $$
       \theta_{i+1} = \theta_i - \alpha \frac{\partial}{\partial \theta_i} J(\theta)
       $$

     - **α：学习率（步长）**不能太大，也不能太小，机器学习中：0.001-0.01

     - **梯度是上升最快的方向，我们需要的是下降最快的方向，所以需要加负号**

     - 步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。**学习率太小**，下降速度会慢；**学习率太大**，容易造成错过最低点、产生下降过程中的震荡、甚至梯度爆炸

   - 梯度下降算法分类：

     - （1）全梯度下降算法（FGD）：每次迭代时，使用全部样本的梯度值

     - $$
       \theta_{i+1} = \theta_i - \alpha \sum_{j=0}^{m} \left( h_{\theta}\left(x_0^{(j)}, x_1^{(j)}, \cdots, x_n^{(j)}\right) - y_j \right) x_i^{(j)}
       $$

     - （2）随机梯度下降算法（SGD）：每次迭代时，随机选择并使用一个样本梯度值

     - $$
       \theta_{i+1} = \theta_i - \alpha \left( h_{\theta}\left(x_0^{(j)}, x_1^{(j)}, \cdots, x_n^{(j)}\right) - y_j \right) x_i^{(j)}
       $$

     - **（3）小批量梯度下降算法（mini-batch）：**每次迭代时，随机选择并使用小批量的样本梯度值，从m个样本中，选择x个样本进行迭代（1<x<m）（目前使用较多）

     - $$
       \theta_{i+1} = \theta_i - \alpha \sum_{j=t}^{t+x-1} \left( h_{\theta}\left(x_0^{(j)}, x_1^{(j)}, \cdots, x_n^{(j)}\right) - y_j \right) x_i^{(j)}
       $$

       

     - （4）随机平均梯度下降算法（SAG）：每次迭代时，随机选择一个样本的梯度值和以往样本的梯度的平均值

     - $$
       \theta_{i+1} = \theta_i - \frac{\alpha}{n} \sum_{j=1}^{n} \left( h_{\theta}\left(x_0^{(j)}, x_1^{(j)}, \cdots, x_n^{(j)}\right) - y_j \right) x_i^{(j)}
       $$

5. **欠拟合和过拟合**

   - ![image-20260101140058841](C:\Users\20538\AppData\Roaming\Typora\typora-user-images\image-20260101140058841.png)

   - **欠拟合：模型在训练集上表现不好，在测试集上表现也不好，模型过于简单**

     - 学习到的数据的特征过少
     - 可以通过添加其他特征、添加多项式特征等方法解决

   - **过拟合：模型在训练集上表现好，在测试集上表现不好，模型过于复杂**

     - 原始特征过多，存在一些嘈杂数据，模型过于复杂是因为模型尝试去兼顾各个测试数据点
     - 可以通过重新清洗数据、增大数据的训练量、**正则化（解决过拟合）**、减少特征维度防止维灾难等方法解决

   - 正则化：在模型训练时，数据中有些特征影响模型复杂度、或者某个特征的异常值较多，所以要尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化

     - 在损失函数中增加正则化项

       - 分别为L1正则化、L2正则化

       - L1正则化：L1正则化会使得权重趋向于0，甚至等于0，使得某些特征失效，达到特征筛选的目的（lasso回归模型）

       - lasso回归L1正则：会把高次方系数变为0

       - $$
         J(w) = \text{MSE}(w) + \alpha \sum_{i=1}^{n} |w_i|
         $$

       - L2正则化：L2正则化会使得权重趋向于0，一般不等于0（岭回归（Ridge Regression）的损失函数）

       - α：惩罚系数越大，权重越；惩罚系数越小，权重越大

       - Ridge线性回归L2正则：不会将系数变为0，但是对高次方项系数影响较大（实际开发中一般倾向使用L2正则化）

       - $$
         J(w) = \text{MSE}(w) + \alpha \sum_{i=1}^{n} w_i^2
         $$


### 3 逻辑回归

1. 逻辑回归的概念：有监督学习，有特征，有标签，且标签是离散的（分类），一般适用于二分类问题

   - 原理：把（线性回归处理后的）值通过**激活函数（Sigmoid）映射到(0，1)(取不到0或者1)之间**，结合阈值来划分正负样本
   - 线性回归的输出作为逻辑回归的输入

2. 逻辑回归的预测函数：

   - $$
     h_\theta(x) = \frac{1}{1 + e^{-\mathbf{\theta}^T \mathbf{x}}}
     $$

     

3. 逻辑回归的损失函数（对数似然损失函数）：

   - 损失函数的工作原理：每个样本的预测值有A，B两个类别，真实类别对应的位置，概率值越大越好

   - $$
     J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
     $$

4. **混淆矩阵**

   - ![image-20260101191629987](C:\Users\20538\AppData\Roaming\Typora\typora-user-images\image-20260101191629987.png)

   - 精确率（Precision）：查准率，对正例样本的预测准确率

     - $$
       \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
       $$

       

   - 召回率（Recall）：查全率，指的是预测为真正例样本占所有真实正例样本的比

     - $$
       \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
       $$

   - F1-score：是精确率和召回率的**调和平均数**，用于综合评估分类模型的性能（平衡精确率与召回率的矛盾）

     - $$
       \text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
       $$

       































































.
